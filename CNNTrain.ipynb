{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from results import Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename: str, model_name: str):\n",
    "    \"\"\"Saves the model to an .h5 file and the model name to a .json file.\n",
    "\n",
    "    Args:\n",
    "        model: Model to be saved\n",
    "        filename: Relative path to the file without the extension.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Save Keras model\n",
    "    model.save(filename + '.h5')\n",
    "\n",
    "    # Save base model information\n",
    "    with open(filename + '.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_name, f, ensure_ascii=False, indent=4, sort_keys=True)\n",
    "def load(self, filename: str):\n",
    "    \"\"\"Loads a trained CNN model and the corresponding preprocessing information.\n",
    "\n",
    "    Args:\n",
    "        filename: Relative path to the file without the extension.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load Keras model\n",
    "    self._model = tf.keras.models.load_model(filename + '.h5')\n",
    "\n",
    "    # Load base model information\n",
    "    with open(filename + '.json') as f:\n",
    "        self._model_name = json.load(f)\n",
    "\n",
    "    self._initialize_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        # reducing/normalizing the pixels\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2) \n",
    "#We dont want the dataset to be flipped as orientation is important, so we delete the horizontal_flip parameter\n",
    "#connecting the image augmentation tool to our dataset\n",
    "\n",
    "targetSizeWidth = 112\n",
    "targetSizeHeight = 112\n",
    "targetSize = (targetSizeWidth, targetSizeHeight)\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "        './training',\n",
    "        target_size = targetSize,\n",
    "        batch_size=128,\n",
    "        class_mode='categorical')\n",
    "\n",
    "#only rescaling but no transformations\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#connecting to the test data\n",
    "val_set = validation_datagen.flow_from_directory(\n",
    "        './validation',\n",
    "        target_size = targetSize,\n",
    "        batch_size = 128,\n",
    "        class_mode = 'categorical')\n",
    "#only rescaling but no transformations\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#connecting to the test data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        './test',\n",
    "        target_size = targetSize,\n",
    "        batch_size = 128,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_training(history):\n",
    "    \"\"\"Plots the evolution of the accuracy and the loss of both the training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        history: Training history.\n",
    "\n",
    "    \"\"\"\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(training_accuracy))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, training_accuracy, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, validation_accuracy, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Building CNN --------------------#\n",
    "# initializing CNN as sequential layers\n",
    "from tf.keras.callbacks import EarlyStopping\n",
    "def create_model():\n",
    "        \n",
    "    cnn = tf.keras.models.Sequential()\n",
    "    kernel_initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    # Step 1: Convolution to get the Feature Map\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters= 64 , kernel_size = 2, activation = 'relu', input_shape=[112,112,3], kernel_initializer=kernel_initializer))\n",
    "    # filters: output feature map\n",
    "    # kernel_size: size of the feature detector\n",
    "    # strides: step size from one filter to the next default is 1\n",
    "    # Step 2: Max Pooling\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 2,padding='same', activation = 'relu'), kernel_initializer=kernel_initializer)\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 128, kernel_size = 2,padding='same', activation = 'relu'), kernel_initializer=kernel_initializer)\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 128, kernel_size = 2,padding='same', activation = 'relu'), kernel_initializer=kernel_initializer)\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    # Step 3: Flattening\n",
    "    cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Step 4: Full Connection\n",
    "    cnn.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))\n",
    "\n",
    "    # Step 5: Output Layer\n",
    "    cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    return cnn\n",
    "\n",
    "#--------------------- Training the CNN --------------------#\n",
    "#compiling the CNN\n",
    "cnn=create_model()\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#training the CNN on the training set and evaluating it on the test set\n",
    "# cnn.fit(x = train_set, validation_data = test_set, epochs = 25)\n",
    "epochs=10\n",
    "# callbacks=\n",
    "# Train the network\n",
    "print(\"\\n\\nTraining CNN...\")\n",
    "# Monitor in real-time the training and validation accuracy and loss\n",
    "early_stopping_monitor = EarlyStopping(patience=4)\n",
    "history = cnn.fit(\n",
    "    train_set,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_set),\n",
    "    validation_data=val_set,\n",
    "    validation_steps=len(val_set),\n",
    "    callbacks=[early_stopping_monitor]\n",
    "    #callbacks=callbacks\n",
    ")\n",
    "if epochs > 1:\n",
    "    _plot_training(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(cnn,'./TRModel','CNN_trainingModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_generator,dataset_name:str, save: bool = True):\n",
    "    \"\"\"Evaluates a new set of images using the trained CNN.\n",
    "\n",
    "    Args:\n",
    "        test_dir: Relative path to the validation directory (e.g., 'dataset/test').\n",
    "        dataset_name: Dataset descriptive name.\n",
    "        save: Save results to an Excel file.\n",
    "\n",
    "    \"\"\"\n",
    "    # Configure loading and pre-processing functions\n",
    "    print('Reading test data...')\n",
    "    # test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=_preprocessing_function)\n",
    "\n",
    "    # test_generator = test_datagen.flow_from_directory(\n",
    "    #     test_dir,\n",
    "    #     target_size=len(test_dir),\n",
    "    #     batch_size=1,  # A batch size of 1 ensures that all test images are processed\n",
    "    #     class_mode='categorical',\n",
    "    #     shuffle=False\n",
    "    # )\n",
    "\n",
    "    # Predict categories\n",
    "    predictions =model.predict(test_generator)\n",
    "    predicted_labels = np.argmax(predictions, axis=1).ravel().tolist()\n",
    "\n",
    "    # Format results and compute classification statistics\n",
    "    results = Results(test_generator.class_indices, dataset_name=dataset_name)\n",
    "    accuracy, confusion_matrix, classification = results.compute(test_generator.filenames, test_generator.classes,\n",
    "                                                                    predicted_labels)\n",
    "    # Display and save results\n",
    "    results.print(accuracy, confusion_matrix)\n",
    "\n",
    "    if save:\n",
    "        results.save(confusion_matrix, classification, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(cnn,test_set,'test_1',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import load_img, img_to_array\n",
    "#--------------------- Single prediction with CNN --------------------#\n",
    "test_image = load_img('./test/JuanO/testJuanO0.jpg', target_size = (112, 112))\n",
    "# to convert image in pii format into a numpy array format\n",
    "test_image = img_to_array(test_image)\n",
    "# adding extra dimension to put this image into a batch by saying where we want to add this batch (as the first dimension)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "# cnn prediction on the test image\n",
    "result = cnn.predict(test_image)\n",
    "# getting the results encoding: which indices correspond to which classes (1: dog, 0:cat)\n",
    "print(train_set.class_indices)\n",
    "\n",
    "#prediction for the single image/element from the batch\n",
    "if result[0][0] == 0:\n",
    "   prediction = 'JuanM'\n",
    "elif result[0][0] == 1:\n",
    "   prediction = 'JuanO'\n",
    "else:\n",
    "   prediction = 'Pablo'\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "test_image2 = load_img('./test/Pablo/testPablo0.jpg', target_size = (640, 480))\n",
    "# to convert image in pii format into a numpy array format\n",
    "test_image2 = img_to_array(test_image2)\n",
    "# adding extra dimension to put this image into a batch by saying where we want to add this batch (as the first dimension)\n",
    "test_image2 = np.expand_dims(test_image2, axis = 0)\n",
    "# cnn prediction on the test image\n",
    "result2 = cnn.predict(test_image2)\n",
    "\n",
    "#prediction for the single image/element from the batch\n",
    "if result[0][0] == 0:\n",
    "   prediction = 'JuanM'\n",
    "elif result[0][0] == 1:\n",
    "   prediction = 'JuanO'\n",
    "else:\n",
    "   prediction = 'Pablo'\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DtNoStructENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec738a6cf87e8c6a3b8a43aaa13e1b2e728a1cfbd14443dc15abb986fd25d83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
