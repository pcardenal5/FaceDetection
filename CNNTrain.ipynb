{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename: str, model_name: str):\n",
    "    \"\"\"Saves the model to an .h5 file and the model name to a .json file.\n",
    "\n",
    "    Args:\n",
    "        model: Model to be saved\n",
    "        filename: Relative path to the file without the extension.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Save Keras model\n",
    "    model.save(filename + '.h5')\n",
    "\n",
    "    # Save base model information\n",
    "    with open(filename + '.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_name, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        # reducing/normalizing the pixels\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2) \n",
    "#We dont want the dataset to be flipped as orientation is important, so we delete the horizontal_flip parameter\n",
    "#connecting the image augmentation tool to our dataset\n",
    "\n",
    "targetSizeWidth = 112\n",
    "targetSizeHeight = 112\n",
    "targetSize = (targetSizeWidth, targetSizeHeight)\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "        './training',\n",
    "        target_size = targetSize,\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "#only rescaling but no transformations\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#connecting to the test data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        './test',\n",
    "        target_size = targetSize,\n",
    "        batch_size = 32,\n",
    "        class_mode = 'categorical')\n",
    "\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Building CNN --------------------#\n",
    "# initializing CNN as sequential layers\n",
    "from tf.keras.callbacks import EarlyStopping\n",
    "def create_model():\n",
    "        \n",
    "    cnn = tf.keras.models.Sequential()\n",
    "\n",
    "    # Step 1: Convolution to get the Feature Map\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters= 210 , kernel_size = 2, activation = 'relu', input_shape=[640,480,3]))\n",
    "    # filters: output feature map\n",
    "    # kernel_size: size of the feature detector\n",
    "    # strides: step size from one filter to the next default is 1\n",
    "    # Step 2: Max Pooling\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    # Step 3: Flattening\n",
    "    cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Step 4: Full Connection\n",
    "    cnn.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))\n",
    "\n",
    "    # Step 5: Output Layer\n",
    "    cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    return cnn\n",
    "\n",
    "#--------------------- Training the CNN --------------------#\n",
    "#compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#training the CNN on the training set and evaluating it on the test set\n",
    "cnn.fit(x = train_set, validation_data = test_set, epochs = 25)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "history = model.fit(X, y, validation_split=0.33, epochs=200, batch_size=15, verbose=0, callbacks=[early_stopping_monitor])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
