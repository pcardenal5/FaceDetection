{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "## Unstructured Data Image proyect\n",
    "### By\n",
    "- Juan Miguel Ramos Pugnaire\n",
    "- Juan Ortega Ortega\n",
    "- Pablo Cardenal\n",
    "\n",
    "All the code is [available on github](https://github.com/pcardenal5/FaceDetection). We have worked mainly on dev, only to merge to main at the end of the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The aim of this project is to create a mock face detection software, in which an interface will be used to interact with the convolutional neural network we will train. It will be able to differentiate between five different classes:\n",
    "\n",
    "- The three faces of all group members (quite a challenge, as we all have a beard and black hair!)\n",
    "- A random face that does not belong to our group.\n",
    "- An image without a face in it.\n",
    "\n",
    "Once the objective is clear we can move on to the description of the project and the challenges we have faced.\n",
    "\n",
    "The report and general workflow of the project has been structured in the following way:\n",
    "## Project Index\n",
    "<ol>\n",
    "<li>Data Extraction</li>\n",
    "<li>Interface development</li>\n",
    "<li>Training & Test</li>\n",
    "<li>Parameter Tuning</li>\n",
    "<li>Results</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Extraction \n",
    "\n",
    "Once the idea for the project was clear the first thing we had to do was getting a good quality dataset. Thankfully there is a plethora of datasets made for face detection and we decided to use the one made by Gwangbin Bae, Martin de La Gorce, et al. on their paper\n",
    "\n",
    "> [DigiFace-1M: 1 Million Digital Face Images for Face Recognition](https://github.com/microsoft/DigiFace1M/raw/main/paper.pdf)\n",
    "\n",
    "The dataset is available for download on [their github page](https://github.com/microsoft/DigiFace1M). As the title of the paper may suggest, this is an artificially generated dataset of faces in which they aim to reduce the problems of dataset crawled from the web. They sey those datasets \"are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise\", as well as posing ethical problems. Thus, we decided to go with a reduced version of this dataset, a third of the original size. Despite this, it takes 3.9 GB. A sample of it can be seen on the following figure:\n",
    "\n",
    "![Screenshot of the sample dataset](FullFaceDataSample.png)\n",
    "\n",
    "Figure 1: Screenshot of the sample dataset.\n",
    "\n",
    "We wanted the model to train with the highest amount of different faces so we took a fragment of the whole dataset, consisting of 5 different expressions per face. This accounted for about 206.000 different pictures, far more than what we will need. \n",
    "\n",
    "This, however, poses another problem as we need to take our pictures in the same format as the chosen dataset; that is, we need to crop a picture of our faces, roughly centered on a 112x112 pixel square. To overcome this problem we created a simple script, `dataCapture/PictueExtracion.py`, that takes care of it. Using the `open-cv` python library we use our laptop webcam and save a 224x224 square that will later be resized. To help centering our faces we overlaid a blue square in the area that will be saved. Originally, we took a 112x112 region but it was difficult and uncomfortable to take pictures. Thus, we decided to take a bigger area and resize it to match the pixel dimensions of the original data.\n",
    "\n",
    "Once the data has been collected we may begin preprocessing it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation \n",
    "*Altering the training and test*  \n",
    "Parameters in data preparation:\n",
    "- directory : Location of the Directory\n",
    "- labels = \"inferred\" : labels of the photos stated by the directory structure\n",
    "- label_mode = \"categorical\" : Type of label\n",
    "- class_names = [\"JuanM\",\"JuanO\",\"Pablo\",\"pepe\"] : Only valid if label is inferred\n",
    "- color_mode = \"grayscale\", \"rgb\", \"rgba\" depending on channel (1,3 or 4)\n",
    "- batch_size = 32 : Number of images that are proccessed in each iteration\n",
    "- image_size = (256,256) -> (default): Size to resize the image when read from disk -> Un batchsize pequeño es más preciso pero más lento \n",
    "- shuffle = True (default)\n",
    "- seed = optional \n",
    "- validation_split = 0.1 : Fraction of data reserved for validation\n",
    "- subset : only used when valdation is set\n",
    "- interpolation = bilinear (default) : used for resizing images \n",
    "- crop_to_aspect_ratio : True para redimensión recortando imagen, False para redimensión con deformidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interface develpment\n",
    "*Interface programming*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training & Test\n",
    "*First model structure*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameter tuning\n",
    "*Detail parameters and how they change*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final Results\n",
    "*Check final results and future implementations*  \n",
    "[Enter explanation]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DtNoStructENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
