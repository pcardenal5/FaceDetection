{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "## Unstructured Data Image proyect\n",
    "### By\n",
    "- Juan Miguel Ramos Pugnaire\n",
    "- Juan Ortega Ortega\n",
    "- Pablo Cardenal\n",
    "\n",
    "All the code is [available on github](https://github.com/pcardenal5/FaceDetection). We have worked mainly on dev, only to merge to main at the end of the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The aim of this project is to create a mock face detection software, in which an interface will be used to interact with the convolutional neural network we will train. It will be able to differentiate between five different classes:\n",
    "\n",
    "- The three faces of all group members (quite a challenge, as we all have a beard and black hair!)\n",
    "- A random face that does not belong to our group.\n",
    "- An image without a face in it.\n",
    "\n",
    "Once the objective is clear we can move on to the description of the project and the challenges we have faced.\n",
    "\n",
    "The report and general workflow of the project has been structured in the following way:\n",
    "### Project Index\n",
    "<ol>\n",
    "<li>Data Extraction</li>\n",
    "<li>Interface development</li>\n",
    "<li>Training & Test</li>\n",
    "<li>Parameter Tuning</li>\n",
    "<li>Results</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Extraction \n",
    "\n",
    "Once the idea for the project was clear the first thing we had to do was getting a good quality dataset. Thankfully there is a plethora of datasets made for face detection and we decided to use the one made by Gwangbin Bae, Martin de La Gorce, et al. on their paper\n",
    "\n",
    "> [DigiFace-1M: 1 Million Digital Face Images for Face Recognition](https://github.com/microsoft/DigiFace1M/raw/main/paper.pdf)\n",
    "\n",
    "The dataset is available for download on [their github page](https://github.com/microsoft/DigiFace1M). As the title of the paper may suggest, this is an artificially generated dataset of faces in which they aim to reduce the problems of dataset crawled from the web. They sey those datasets \"are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise\", as well as posing ethical problems. Thus, we decided to go with a reduced version of this dataset, a third of the original size. Despite this, it takes 3.9 GB. A sample of it can be seen on the following figure:\n",
    "\n",
    "![Screenshot of the sample dataset](FullFaceDataSample.png)\n",
    "\n",
    "Figure 1: Screenshot of the 'Random Face' dataset. Daunting, to say the least.\n",
    "\n",
    "We wanted the model to train with the highest amount of different faces so we took a fragment of the whole dataset, consisting of 5 different expressions per face/individual. This accounted for about 206.000 different pictures, far more than what we will need. \n",
    "\n",
    "This, however, poses another problem as we need to take our pictures in the same format as the chosen dataset; that is, we need to crop a picture of our faces, roughly centered on a 112x112 pixel square. To overcome this problem we created a simple script, `dataCapture/PictueExtracion.py`, that takes care of it. Using the `open-cv` python library we use our laptop webcam and save a 224x224 square that will later be resized. To help centering our faces we overlaid a blue square in the area that will be saved. Originally, we took a 112x112 region but it was difficult and uncomfortable to take pictures. Thus, we decided to take a bigger area and resize it to match the pixel dimensions of the original data. A sample of one of those face groups can be seen on the following figure:\n",
    "\n",
    "![Screenshot of sample data for Pablo's faces.](PabloDataSample.png)\n",
    "\n",
    "Figure 2: Screenshot of sample data for Pablo's faces.\n",
    "\n",
    "Obviously, this are not the only images the network was fed. We have taken about 10.000 photographs each with idfferent backgrounds, distance to the camera and lightning; both in a direct light such as the torch from our phones or the light reflected off a wall. The aim is to create a very diverse set of images similar to the input data of the paper.\n",
    "\n",
    "Using this data, and a similar set of pictures for the other group members, we have to take care of the final output class: pictures with no faces on it. We have decided to use [this dataset](https://data.caltech.edu/records/nyy15-4j048) provided by Caltech that contains 256 different classes. We have need to clean some of those, though, because some contain human faces and we don't want the cnn to get confused.\n",
    "\n",
    "Once the data has been collected we may begin preprocessing it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation \n",
    "\n",
    "In regards of data preparation and preprocessing, we did not really need to do it. That is because all of the input data has been selected/handcrafted to fit the needs of the big dataset presented in figure 1. As such, we only needed to resize the Caltech data set to fit the pixel dimensions of both datasets. This has been done rather easily using the implemented `flow_from_directory` function. In addition to this, following what has been said on the previous section, we needed to select some clases to be deleted from the Caltech dataset for they contained faces. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interface development\n",
    "\n",
    "To develop the interface we have used the `PySimpleGUI` python library. It contains many built-in functions to create interfaces and we have used it as a way to test in near Real Time the abilities of our neural network. It is a rather simple interface; half of the window is composed of the image captured by the webcam and the other half contains a list of the posible output classes. They have a 'light' that indicates the output of the network, one for each output class defined at the start of the report. The interface captures a section of the image from the webcam, indicated by a square on the image, and feeds it to the network, Then, once the prediction is made, the lights update accordingly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training & Test\n",
    "\n",
    "First, let's start with the creation of the convolutional network, which we will then use. The architecture of the network is as follows:\n",
    "- Four convolutional blocks, each containing a convolutional layer followed by a max-pooling layer. In each pooling layer, a 2x2 window and a stride of 2 is used to reduce the dimensionality of the input image.\n",
    "- The convolutional blocks are followed by the flatten layer which converts the output of the last max-pooling layer a 3D tensor, into a 1D array.\n",
    "- For a more accurate feature extraction, just before the classification layer we have included a dense layer whose activation function is `relu` and has 512 neurons.\n",
    "- The last dense layer has 5 neurons, as we have 5 classes, and uses the sigmoid activation function, which makes it suitable for multi-label classification problems as other activation functions do not create a probability distribution.\n",
    "\n",
    "Then, the training, validation and test directories are established:\n",
    "- Train: Data augmentation transformations such as random rotations, shifts and zooms are applied to increase the variability of the data.\n",
    "- Validation: Only a scaling of the image pixel values is performed.\n",
    "- Test: Only a scaling of the image pixel values is performed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameter tuning\n",
    "*Detail parameters and how they change*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final Results\n",
    "*Check final results and future implementations* \n",
    " \n",
    "[Enter explanation]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DtNoStructENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
