{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "## Unstructured Data Image proyect\n",
    "### By\n",
    "- Juan Miguel Ramos Pugnaire\n",
    "- Juan Ortega Ortega\n",
    "- Pablo Cardenal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The aim of this project is to create a mock face detection software, in which an interface will be used to interact with the convolutional neural network we will train. It will be able to differentiate between five different classes:\n",
    "\n",
    "- The three faces of all group members (quite a challegene, as we all have a beard and black hair!)\n",
    "- A random face that does not belong to our group.\n",
    "- An image without a face in it.\n",
    "\n",
    "Once the objective is clear we can move on to the description of the project and the challenges we have faced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "The report and general workflow of the project has been structured in the following way:\n",
    "\n",
    "__Project Index__\n",
    "<ol>\n",
    "<li>Data Extraction</li>\n",
    "<li>Interface development</li>\n",
    "<li>Training & Test</li>\n",
    "<li>Parameter Tuning</li>\n",
    "<li>Results</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "training_files=\"./training\"\n",
    "test_files=\"./test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Extraction \n",
    "\n",
    "Once the idea for the project was clear the first thing we had to do was getting a good quality dataset. Thankfully there is a plethora of datasets made for fface detection and we decided to use the one made by Gwangbin Bae, Martin de La Gorce, et al. on their paper\n",
    "\n",
    "> [DigiFace-1M: 1 Million Digital Face Images for Face Recognition](https://github.com/microsoft/DigiFace1M/raw/main/paper.pdf)\n",
    "\n",
    "the dataset is available for download on [their github page](https://github.com/microsoft/DigiFace1M). As the title of the paper may suggest, this is an artificially generated dataset of faces in which they aim to reduce the problems of dataset crawled from the web. They sey those datasets \"are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise\", as well as posing ethical problems. Thus, we decided to go with a reduced version of this dataset. A sample of it can be seen on the following figure:\n",
    "\n",
    "![Screenshot of the sample dataset](FullFaceDataSample.png)\n",
    "\n",
    "Figure 1: Screenshot of the sample dataset.\n",
    "\n",
    "We wanted the model to train with the highest amount of different faces so we took a fragment of the whole dataset, consisting of 5 different expressions per face. This accounted for about 206.000 different pictures, far more than what we will need. \n",
    "\n",
    "This, however, poses another problem as we need to take our pictures in the same format as the chosen dataset; that is, we need to crop a picture of our faces, roughfly centered on a 112x112 pixel square. To overcome this problem we created a simple script, `dataCapture/PictueExtracion.py`, that takes care of it. using the `open-cv` python library we use our laptop webcam and save a 224x224 square that will later be resized. To help centering our faces we overlaid a blue square in the area that will later be saved. Originally, we took a 112x112 region but it was difficult and uncomfortable to take pictures. Thus, we decided to take a bigger area and resize it to fit the pixel dimensions of the original data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation \n",
    "*Altering the training and test*  \n",
    "Parameters in data preparation:\n",
    "- directory : Location of the Directory\n",
    "- labels=\"inferred\": labels of the photos stated by the directory structure\n",
    "- label_mode= \"categorical\" : Type of label\n",
    "- class_names= [\"JuanM\",\"JuanO\",\"Pablo\",\"pepe\"]: Only valid if label is inferred\n",
    "- color_mode= \"grayscale\", \"rgb\", \"rgba\" depending on channel\n",
    "- batch_size=32 : Number of images tat are proccessed in each iteration\n",
    "- image_size=(256,256)(default): Size to resize the image when rea from disk-> Un batchsize pequeño es más preciso pero más lento \n",
    "- shuffle=True(default)\n",
    "- seed=optional \n",
    "- validation_split= 0.1 :Fraction of data reserved for validation\n",
    "- subset: only used when valdation is set\n",
    "- interpolation=bilinear(default) : used for resizing images \n",
    "- crop_to_aspect_ratio: True para redimensión recortando imagen, False para redimensión con deformidad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename: str, model_name: str):\n",
    "    \"\"\"Saves the model to an .h5 file and the model name to a .json file.\n",
    "\n",
    "    Args:\n",
    "        filename: Relative path to the file without the extension.\n",
    "\n",
    "    \"\"\"\n",
    "    # Save Keras model\n",
    "    model.save(filename + '.h5')\n",
    "\n",
    "    # Save base model information\n",
    "    with open(filename + '.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_name, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "#--------------------- Data Preprocessing --------------------#\n",
    "\n",
    "#feature training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        # reducing/normalizing the pixels\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "#connecting the image augmentation tool to our dataset\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "        './training',\n",
    "        #final size of the images that will be fed into the ann\n",
    "        target_size=(640, 480),\n",
    "        # number of images that we want to have in each batch\n",
    "        batch_size=32,\n",
    "        # we have binary classification --> binary class mode\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "#only rescaling but no transformations\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#connecting to the test data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        './test',\n",
    "        target_size=(640, 480),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "print(test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interface develpment\n",
    "*Interface programming*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training & Test\n",
    "*First model structure*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Building CNN --------------------#\n",
    "# initializing CNN as sequential layers\n",
    "from tf.keras.callbacks import EarlyStopping\n",
    "def create_model():\n",
    "        \n",
    "    cnn = tf.keras.models.Sequential()\n",
    "\n",
    "    # Step 1: Convolution to get the Feature Map\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters= 210 , kernel_size = 2, activation = 'relu', input_shape=[640,480,3]))\n",
    "    # filters: output feature map\n",
    "    # kernel_size: size of the feature detector\n",
    "    # strides: step size from one filter to the next default is 1\n",
    "    # Step 2: Max Pooling\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    #adding a second convolutional layer\n",
    "    cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 ,strides=2))\n",
    "    # Step 3: Flattening\n",
    "    cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Step 4: Full Connection\n",
    "    cnn.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))\n",
    "\n",
    "    # Step 5: Output Layer\n",
    "    cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    return cnn\n",
    "\n",
    "#--------------------- Training the CNN --------------------#\n",
    "#compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#training the CNN on the training set and evaluating it on the test set\n",
    "cnn.fit(x = train_set, validation_data = test_set, epochs = 25)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "history = model.fit(X, y, validation_split=0.33, epochs=200, batch_size=15, verbose=0, callbacks=[early_stopping_monitor])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameter tuning\n",
    "*Detail parameters and how they change*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final Results\n",
    "*Check final results and future implementations*  \n",
    "[Enter explanation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DtNoStructENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
